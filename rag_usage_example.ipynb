{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSeq2SeqLM, AutoModelForCausalLM\n",
    "from typing import List, Union, Dict, Tuple, Any, Optional\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "nltk.download('punkt', quiet=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define generalized RAG class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LLMQuestionAnswerer:\n",
    "    def __init__(self, \n",
    "                 model_name: str, \n",
    "                 model_type: str = \"seq2seq\",\n",
    "                 quest_ans_tokens_margin: int = 100):\n",
    "        self.model_name = model_name\n",
    "        self.model_type = model_type\n",
    "        self.quest_ans_tokens_margin = quest_ans_tokens_margin\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(self.model_name,\n",
    "                                                       clean_up_tokenization_spaces=True)\n",
    "        \n",
    "        if model_type == \"seq2seq\":\n",
    "            self.model = AutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n",
    "        elif model_type == \"causal\":\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(self.model_name)\n",
    "        else:\n",
    "            raise ValueError(\"Unsupported model type. Use 'seq2seq' or 'causal'.\")\n",
    "        \n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model.to(self.device)\n",
    "        \n",
    "        self.rag_data: List[str] = []\n",
    "        self.rag_embeddings: Optional[torch.Tensor] = None\n",
    "        self.embedding_size = self.model.config.hidden_size\n",
    "        self.rag_k_nearest_neighbors = 3\n",
    "        # self.max_chunk_size = self.get_max_chunk_size()\n",
    "\n",
    "    @property\n",
    "    def max_chunk_size(self) -> int:\n",
    "        \"\"\"\n",
    "        Determine the maximum chunk size based on the model's maximum input length.\n",
    "        \"\"\"\n",
    "        return self.model.config.__dict__.get(\"max_position_embeddings\", 500) - self.quest_ans_tokens_margin  # Leave some margin for question and answer\n",
    "\n",
    "    def split_document(self, document: str) -> List[str]:\n",
    "        \"\"\"\n",
    "        Split a document into chunks that fit within the model's context window.\n",
    "        \"\"\"\n",
    "        sentences = sent_tokenize(document)\n",
    "        chunks = []\n",
    "        current_chunk = []\n",
    "        current_length = 0\n",
    "\n",
    "        for sentence in sentences:\n",
    "            sentence_tokens = self.tokenizer.encode(sentence)\n",
    "            sentence_length = len(sentence_tokens)\n",
    "\n",
    "            if current_length + sentence_length > self.max_chunk_size:\n",
    "                if current_chunk:\n",
    "                    chunks.append(\" \".join(current_chunk))\n",
    "                current_chunk = [sentence]\n",
    "                current_length = sentence_length\n",
    "            else:\n",
    "                current_chunk.append(sentence)\n",
    "                current_length += sentence_length\n",
    "\n",
    "        if current_chunk:\n",
    "            chunks.append(\" \".join(current_chunk))\n",
    "\n",
    "        return chunks\n",
    "\n",
    "    def summarize_chunk(self, chunk: str, max_length: int = 100) -> str:\n",
    "        \"\"\"\n",
    "        Generate a summary for a given chunk of text.\n",
    "        \"\"\"\n",
    "        summary_prompt = \"Can you provide a comprehensive summary of the given text? \\\n",
    "                          The summary should cover all the key points and main ideas presented in the original text, \\\n",
    "                          while also condensing the information into a concise and easy-to-understand format. \\\n",
    "                          Please ensure that the summary includes relevant details and examples that support the main ideas, \\\n",
    "                          while avoiding any unnecessary information or repetition:\"\n",
    "        inputs = self.tokenizer(summary_prompt + chunk, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.model_type == \"seq2seq\":\n",
    "                summary_ids = self.model.generate(**inputs, max_length=max_length)\n",
    "            else:  # causal\n",
    "                summary_ids = self.model.generate(**inputs, max_length=inputs['input_ids'].shape[1] + max_length)\n",
    "        \n",
    "        summary = self.tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "        return summary\n",
    "\n",
    "    def get_embedding(self, text: Union[str, List[str]]) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Generate embeddings for the given text or list of texts using the main model.\n",
    "        \"\"\"\n",
    "        if isinstance(text, str):\n",
    "            text = [text]\n",
    "        \n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True, max_length=512).to(self.device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            if self.model_type == \"seq2seq\":\n",
    "                outputs = self.model.encoder(**inputs)\n",
    "            else:  # causal\n",
    "                outputs = self.model(**inputs)\n",
    "        \n",
    "        # Use mean pooling to get a single vector per text\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.cpu()\n",
    "\n",
    "    def add_rag_data(self, documents: List[str], use_summaries: bool = False):\n",
    "        \"\"\"\n",
    "        Add data for Retrieval-Augmented Generation (RAG), automatically splitting documents and computing embeddings.\n",
    "        :param documents: List of documents to add to the RAG database.\n",
    "        :param use_summaries: Whether to use chunk summaries for embeddings instead of raw chunks.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for doc in documents:\n",
    "            chunks = self.split_document(doc)\n",
    "            all_chunks.extend(chunks)\n",
    "\n",
    "        if use_summaries:\n",
    "            summaries = [self.summarize_chunk(chunk) for chunk in tqdm(all_chunks, desc=\"Generating summaries\")]\n",
    "            self.rag_data.extend(summaries)\n",
    "            new_embeddings = self.get_embedding(summaries)\n",
    "        else:\n",
    "            self.rag_data.extend(all_chunks)\n",
    "            new_embeddings = self.get_embedding(all_chunks)\n",
    "        \n",
    "        if self.rag_embeddings is None:\n",
    "            self.rag_embeddings = new_embeddings\n",
    "        else:\n",
    "            self.rag_embeddings = torch.cat([self.rag_embeddings, new_embeddings], dim=0)\n",
    "        \n",
    "        print(f\"Added {len(all_chunks)} chunks from {len(documents)} documents to the RAG database. Total chunks: {len(self.rag_data)}\")\n",
    "\n",
    "    def get_relevant_context(self, question: str) -> str:\n",
    "        \"\"\"\n",
    "        Retrieve relevant context from RAG data using cosine similarity.\n",
    "        :param question: The question to find context for.\n",
    "        :param k: Number of most relevant contexts to retrieve.\n",
    "        :return: String of relevant contexts.\n",
    "        \"\"\"\n",
    "        if self.rag_embeddings is None:\n",
    "            return \"\"\n",
    "        \n",
    "        question_embedding = self.get_embedding(question)\n",
    "        \n",
    "        # Compute cosine similarity\n",
    "        similarities = torch.cosine_similarity(question_embedding, self.rag_embeddings)\n",
    "        \n",
    "        # Get top k similar contexts\n",
    "        _, indices = similarities.topk(self.rag_k_nearest_neighbors)\n",
    "        \n",
    "        relevant_chunks = [self.rag_data[i] for i in indices]\n",
    "        return \" \".join(relevant_chunks)\n",
    "\n",
    "    def answer_question(self, \n",
    "                        prompt: str, \n",
    "                        use_rag: bool = False, \n",
    "                        max_length: int = 100, \n",
    "                        output_structure: Dict[str, Any] = None) -> Union[str, Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Answer a question based on the given prompt, optionally using RAG and output structure.\n",
    "        :param prompt: The question or prompt to answer.\n",
    "        :param use_rag: Whether to use Retrieval-Augmented Generation.\n",
    "        :param max_length: Maximum length of the generated answer.\n",
    "        :param output_structure: Dictionary specifying the desired output structure and field types.\n",
    "        :return: The generated answer as a string or a structured dictionary.\n",
    "        \"\"\"\n",
    "        if use_rag:\n",
    "            context = self.get_relevant_context(prompt)\n",
    "            full_prompt = f\"Context: {context}\\n\\nQuestion: {prompt}\\n\\nAnswer:\"\n",
    "        else:\n",
    "            full_prompt = f\"Question: {prompt}\\n\\nAnswer:\"\n",
    "\n",
    "        if output_structure:\n",
    "            return self.generate_structured_output(full_prompt, output_structure, max_length)\n",
    "\n",
    "        try:\n",
    "            inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.model_type == \"seq2seq\":\n",
    "                    outputs = self.model.generate(**inputs, max_length=max_length)\n",
    "                else:  # causal\n",
    "                    outputs = self.model.generate(**inputs, max_length=inputs['input_ids'].shape[1] + max_length)\n",
    "            \n",
    "            return self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        except Exception as e:\n",
    "            return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "    def generate_structured_output(self, prompt: str, output_structure: Dict[str, Any], max_length: int = 200) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Generate a structured output based on the given prompt and desired structure.\n",
    "        :param prompt: The input prompt.\n",
    "        :param output_structure: Dictionary specifying the desired output structure and field types.\n",
    "        :param max_length: Maximum length of the generated answer.\n",
    "        :return: A dictionary with the structured output.\n",
    "        \"\"\"\n",
    "        structure_prompt = json.dumps(output_structure, indent=2)\n",
    "        full_prompt = f\"{prompt}\\n\\nGenerate a response in the following JSON structure:\\n{structure_prompt}\\nResponse:\"\n",
    "\n",
    "        try:\n",
    "            inputs = self.tokenizer(full_prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(self.device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                if self.model_type == \"seq2seq\":\n",
    "                    outputs = self.model.generate(**inputs, max_length=max_length)\n",
    "                else:  # causal\n",
    "                    outputs = self.model.generate(**inputs, max_length=inputs['input_ids'].shape[1] + max_length)\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            \n",
    "            # Try to parse the response as JSON\n",
    "            try:\n",
    "                structured_output = json.loads(response)\n",
    "                # return self.validate_and_convert_types(structured_output, output_structure)\n",
    "                return structured_output\n",
    "            except json.JSONDecodeError:\n",
    "                return {\"error\": \"Failed to generate valid JSON structure\", \"raw_response\": response}\n",
    "\n",
    "        except Exception as e:\n",
    "            return {\"error\": f\"An error occurred: {str(e)}\"}\n",
    "\n",
    "    # def validate_and_convert_types(self, generated_output: Dict[str, Any], desired_structure: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    #     \"\"\"\n",
    "    #     Validate and convert the types of the generated output to match the desired structure.\n",
    "    #     :param generated_output: The generated output dictionary.\n",
    "    #     :param desired_structure: The desired output structure with type annotations.\n",
    "    #     :return: A dictionary with validated and converted types.\n",
    "    #     \"\"\"\n",
    "    #     # ... (implementation remains unchanged)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u> Use LLM:</u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_system = LLMQuestionAnswerer(\"google/flan-t5-base\", model_type=\"seq2seq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simple question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'london'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_system.answer_question(\"What is the capital of France?\", max_length=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 5 chunks from 5 documents to the RAG database. Total chunks: 5\n"
     ]
    }
   ],
   "source": [
    "qa_system.add_rag_data([\n",
    "    \"Paris is the largest and most important city in France.\",\n",
    "    \"The Eiffel Tower is located in Paris.\",\n",
    "    \"France is known for its cuisine, including croissants and baguettes.\",\n",
    "    \"The Louvre Museum in Paris houses the Mona Lisa painting.\",\n",
    "    \"French is the official language of France.\",\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_system.answer_question(\"What is the capital of France?\", use_rag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris is the largest and most important city in France'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_system.answer_question(\"Tell me about Paris in two sentences\", use_rag=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gen_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
